- How can you scale it to improve throughput?
    - Horizontal scaling
    - Multiple Brokers to handle producers and consumers
    - Multiple Partitions
    - Consumer Groups with multiple consumers consuming data from different partitions

- Explain how you would cope with failure if the app crashes mid day / mid year.
    - DLQ in case of exception at consumer side
    - We can set enable.auto.commit as False but then we need to manually commit after each iteration of data


- When creating e.g. per minute statistics, how do you handle frames that arrive late or frames with a random timestamp (e.g. hit by a bitflip), describe a strategy?
      - As each event has a time stamp, strategy for this should be to wait before producing the result. Currently, we are publishing the result every 5 seconds. To handle the above scenario, 	we should store the data for 5 seconds but should not publish it right away. We should add some pre-defined extra time to handle late events and should produce the results accordingly.

- Ability to write performant code to handle streaming data. measure and document how fast your solution is.
    - Almost 2 seconds to generate metrics (per minute, week, month, year) every 5 seconds and push to Kafka on local machine 
- Scalability: explain how you would scale your approach
    - Horizontal scaling
    - Multiple Brokers to handle producers and consumers
    - Multiple Partitions
    - Consumer Groups with multiple consumers consuming data from different partitions 
- Benchmark?
    - No tools were used. Counted the total consumed data from Kafka and printed after every iteration 
    - on average 3000-4000 events every 5 seconds on local machine


